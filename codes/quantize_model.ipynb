{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tangotew/miniconda3/envs/llm_quant/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.quantization\n",
    "import torch\n",
    "\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from codes.helpers import load_and_save_model, load_and_quantize_gpt2model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and directories\n",
    "# model_name = \"microsoft/DialoGPT-medium\"\n",
    "# model_dir = \"../models/DialoGPT-medium\"\n",
    "# Define model names and directories\n",
    "model_name = \"gpt2\"\n",
    "model_dir = \"../models/gpt2\"\n",
    "quantized_model_dir = \"../models/quantized_DialoGPT-medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and test the unquantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer already exist in ../models/gpt2\n",
      "Input IDs: tensor([[17250,    11,   703,   389,   345,    30]])\n",
      "Unquantized model response: Hi, how are you? I'm gonna go eat some food next.\n",
      "\n",
      "\n",
      "Might as well make it a whole lot easier for people to use my iPhone in the future when they get their hands on an iOS app! (A: if this is what makes your iPad feel so comfortable while reading) Thank u very much and thank y'all again :)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and save the unquantized model\n",
    "unquantized_model, tokenizer = load_and_save_model(model_name, model_dir)\n",
    "\n",
    "# Test the unquantized model\n",
    "input_text = \"Hi, how are you?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Print the input IDs to verify\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "\n",
    "# Create an attention mask\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "# Simplify the generation call to isolate issues\n",
    "response_ids = unquantized_model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    "    do_sample=True,      # Enable sampling to allow diverse outputs\n",
    "    top_k=50,            # Consider the top 50 tokens at each step\n",
    "    top_p=0.95,          # Use nucleus sampling\n",
    "    temperature=0.9,     # Control the randomness of predictions\n",
    "    num_return_sequences=1,  # Generate one response\n",
    "    repetition_penalty=2.0   # Penalize repetitions\n",
    ")\n",
    "\n",
    "# Print the response IDs to verify\n",
    "# print(f\"Response IDs: {response_ids}\")\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "print(f\"Unquantized model response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model and tokenizer already exist in ../models/quantized_gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tangotew/miniconda3/envs/llm_quant/lib/python3.9/site-packages/torch/_utils.py:382: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "quantized_model_dir = \"../models/quantized_gpt2\"\n",
    "# Now let's quantize the model and test the quantized version\n",
    "q_model, q_tokenizer = load_and_quantize_gpt2model(model_name, model_dir, quantized_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Response IDs: tensor([[17250,    11,   703,   389,   345,    30,  4231,   612,   597,   517,\n",
      "          2683,   878,   314,   923, 11170,   284,   262, 10650,   644,   318,\n",
      "           290,  2125,   470,   281,  7950,  1701,   198,  2504,   338,   618,\n",
      "           616,  4957,  1625,   287,    13,   317,  1178,  2431,  1568,   257,\n",
      "          2415,   508,   373,   379,   607,  2802,    12,   259, 20977,  6270,\n",
      "          1297,   502,   326,   673,   550,  2982,   546,   428, 10241,  1141,\n",
      "           674,  1561,  2961,   319,  3909,  1755,   981,  6155,  1363,   422,\n",
      "           670,  2111,   407,   307, 32064,   416,   514,  2282,   366,    40,\n",
      "          1392, 10423,   938,  3502,   351, 20345,  2474,   770,   561,  1283,\n",
      "           523,  5629,   706,   356,  1053,   925,   510,  1223,   649,    25]])\n",
      "Quantized model response: Hi, how are you? Are there any more questions before I start explaining to the guests what is and isn't an abortion?\"\n",
      "That's when my daughter came in. A few minutes later a woman who was at her mother-inâ€‘law told me that she had heard about this pregnancy during our talk earlier on Saturday night while walking home from work trying not be alarmed by us saying \"I got pregnant last Sunday with twins!\" This would seem so odd after we've made up something new:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the quantized model\n",
    "response_ids = q_model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,\n",
    "    do_sample=True,      # Enable sampling to allow diverse outputs\n",
    "    top_k=50,            # Consider the top 50 tokens at each step\n",
    "    top_p=0.95,          # Use nucleus sampling\n",
    "    temperature=0.9,     # Control the randomness of predictions\n",
    "    num_return_sequences=1,  # Generate one response\n",
    "    repetition_penalty=2.0   # Penalize repetitions\n",
    ")\n",
    "\n",
    "# Print the response IDs to verify\n",
    "print(f\"Quantized Response IDs: {response_ids}\")\n",
    "\n",
    "# Decode the response\n",
    "quantized_response = q_tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "print(f\"Quantized model response: {quantized_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
